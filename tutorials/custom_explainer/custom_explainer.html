

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Tutorial - Implementing a custom analysis block in Lightwood &mdash; lightwood 23.2.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/mindsdblogo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                23.2.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Tutorials</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">API</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Data</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../encoder.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Encoders</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mixer.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Mixers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ensemble.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Ensemble</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analysis.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Analysis</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../helpers.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Helpers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lightwood_philosophy.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Lightwood</span> <span class="pre">Philosophy</span></code></a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">lightwood</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Tutorial - Implementing a custom analysis block in Lightwood</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/tutorials/custom_explainer/custom_explainer.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Tutorial---Implementing-a-custom-analysis-block-in-Lightwood">
<h1>Tutorial - Implementing a custom analysis block in Lightwood<a class="headerlink" href="#Tutorial---Implementing-a-custom-analysis-block-in-Lightwood" title="Permalink to this headline">¶</a></h1>
<div class="section" id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h2>
<p>As you might already know, Lightwood is designed to be a flexible machine learning (ML) library that is able to abstract and automate the entire ML pipeline. Crucially, it is also designed to be extended or modified very easily according to your needs, essentially offering the entire spectrum between fully automated AutoML and a lightweight wrapper for customized ML pipelines.</p>
<p>As such, we can identify several different customizable “phases” in the process. The relevant phase for this tutorial is the “analysis” that comes after a predictor has been trained. The goal of this phase is to generate useful insights, like accuracy metrics, confusion matrices, feature importance, etc. These particular examples are all included in the core analysis procedure that Lightwood executes.</p>
<p>However, the analysis procedure is structured into a sequential execution of “analysis blocks”. Each analysis block should generate a well-defined set of insights, as well as handling any actions regarding these at inference time.</p>
<p>As an example, one of the core blocks is the Inductive Conformal Prediction (<code class="docutils literal notranslate"><span class="pre">ICP</span></code>) block, which handles the confidence estimation of all Lightwood predictors. The logic within can be complex at times, but thanks to the block abstraction we can deal with it in a structured manner. As this <code class="docutils literal notranslate"><span class="pre">ICP</span></code> block is used when generating predictions, it implements the two main methods that the <code class="docutils literal notranslate"><span class="pre">BaseAnalysisBlock</span></code> class specifies: <code class="docutils literal notranslate"><span class="pre">.analyze()</span></code> to setup everything that is needed, and <code class="docutils literal notranslate"><span class="pre">.explain()</span></code> to
actually estimate the confidence in any given prediction.</p>
</div>
<div class="section" id="Objective">
<h2>Objective<a class="headerlink" href="#Objective" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we will go through the steps required to implement your own analysis blocks to customize the insights of any Lightwood predictor!</p>
<p>In particular, we will implement a “model correlation heatmap” block: we want to compare the predictions of all mixers inside a <code class="docutils literal notranslate"><span class="pre">BestOf</span></code> ensemble object, to understand how they might differ in their overall behavior.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">lightwood</span>
<span class="n">lightwood</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-2216:No torchvision detected, image helpers not supported.</span>
<span class="ansi-green-fg">INFO:lightwood-2216:No torchvision/pillow detected, image encoder not supported</span>
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;23.2.1.0&#39;
</pre></div></div>
</div>
</div>
<div class="section" id="Step-1:-figuring-out-what-we-need">
<h2>Step 1: figuring out what we need<a class="headerlink" href="#Step-1:-figuring-out-what-we-need" title="Permalink to this headline">¶</a></h2>
<p>When designing an analysis block, an important choice needs to be made: will this block operate when calling the predictor? Or is it only going to describe its performance once in the held-out validation dataset?</p>
<p>Being in the former case means we need to implement both <code class="docutils literal notranslate"><span class="pre">.analyze()</span></code> and <code class="docutils literal notranslate"><span class="pre">.explain()</span></code> methods, while the latter case only needs an <code class="docutils literal notranslate"><span class="pre">.analyze()</span></code> method. Our <code class="docutils literal notranslate"><span class="pre">ModelCorrelationHeatmap</span></code> belongs to this second category.</p>
<p>Let’s start the implementation by inheriting from <code class="docutils literal notranslate"><span class="pre">BaseAnalysisBlock</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightwood.analysis</span> <span class="kn">import</span> <span class="n">BaseAnalysisBlock</span>

<span class="k">class</span> <span class="nc">ModelCorrelationHeatmap</span><span class="p">(</span><span class="n">BaseAnalysisBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deps</span><span class="o">=</span><span class="nb">tuple</span><span class="p">()):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">deps</span><span class="o">=</span><span class="n">deps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">analyze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">explain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">row_insights</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                <span class="n">global_insights</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]]:</span>

        <span class="k">return</span> <span class="n">row_insights</span><span class="p">,</span> <span class="n">global_insights</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ModelCorrelationHeatmap</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;__main__.ModelCorrelationHeatmap at 0x7fdbebf4da30&gt;
</pre></div></div>
</div>
<p>Right now, our newly created analysis block doesn’t do much, apart from returning the <code class="docutils literal notranslate"><span class="pre">info</span></code> and insights (<code class="docutils literal notranslate"><span class="pre">row_insights</span></code> and <code class="docutils literal notranslate"><span class="pre">global_insights</span></code>) exactly as it received them from the previous block.</p>
<p>As previously discussed, we only need to implement a procedure that runs post-training, no action is required at inference time. This means we can use the default <code class="docutils literal notranslate"><span class="pre">.explain()</span></code> behavior in the parent class:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ModelCorrelationHeatmap</span><span class="p">(</span><span class="n">BaseAnalysisBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deps</span><span class="o">=</span><span class="nb">tuple</span><span class="p">()):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">deps</span><span class="o">=</span><span class="n">deps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">analyze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">info</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Step-2:-Implementing-the-custom-analysis-block">
<h2>Step 2: Implementing the custom analysis block<a class="headerlink" href="#Step-2:-Implementing-the-custom-analysis-block" title="Permalink to this headline">¶</a></h2>
<p>Okay, now for the fun bit: we have to implement a correlation heatmap between the predictions of all mixers inside a <code class="docutils literal notranslate"><span class="pre">BestOf</span></code> ensemble. This is currently the only ensemble implemented in Lightwood, but it is a good idea to explicitly check that the type of the ensemble is what we expect.</p>
<p>A natural question to ask at this point is: what information do we have to implement the procedure? You’ll note that, apart from the <code class="docutils literal notranslate"><span class="pre">info</span></code> dictionary, we receive a <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> dictionary. You can check out the full documentation for more details, but the keys (and respective value types) exposed in this object by default are:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;predictor&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.ensemble.BaseEnsemble&#39;</span><span class="p">,</span>
        <span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="s1">&#39;str&#39;</span><span class="p">,</span>
        <span class="s1">&#39;input_cols&#39;</span><span class="p">:</span> <span class="s1">&#39;list&#39;</span><span class="p">,</span>
        <span class="s1">&#39;dtype_dict&#39;</span><span class="p">:</span> <span class="s1">&#39;dict&#39;</span><span class="p">,</span>
        <span class="s1">&#39;normal_predictions&#39;</span><span class="p">:</span> <span class="s1">&#39;pd.DataFrame&#39;</span><span class="p">,</span>
        <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="s1">&#39;pd.DataFrame&#39;</span><span class="p">,</span>
        <span class="s1">&#39;train_data&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.data.encoded_ds.EncodedDs&#39;</span><span class="p">,</span>
        <span class="s1">&#39;encoded_val_data&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.data.encoded_ds.EncodedDs&#39;</span><span class="p">,</span>
        <span class="s1">&#39;is_classification&#39;</span><span class="p">:</span> <span class="s1">&#39;bool&#39;</span><span class="p">,</span>
        <span class="s1">&#39;is_numerical&#39;</span><span class="p">:</span> <span class="s1">&#39;bool&#39;</span><span class="p">,</span>
        <span class="s1">&#39;is_multi_ts&#39;</span><span class="p">:</span> <span class="s1">&#39;bool&#39;</span><span class="p">,</span>
        <span class="s1">&#39;stats_info&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.api.types.StatisticalAnalysis&#39;</span><span class="p">,</span>
        <span class="s1">&#39;ts_cfg&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.api.types.TimeseriesSettings&#39;</span><span class="p">,</span>
        <span class="s1">&#39;accuracy_functions&#39;</span><span class="p">:</span> <span class="s1">&#39;list&#39;</span><span class="p">,</span>
        <span class="s1">&#39;has_pretrained_text_enc&#39;</span><span class="p">:</span> <span class="s1">&#39;bool&#39;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>As you can see there is lots to work with, but for this example we will focus on using:</p>
<ol class="arabic simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">predictor</span></code> ensemble</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">encoded_val_data</span></code> to generate predictions for each mixer inside the ensemble</p></li>
</ol>
<p>And the insight we’re want to produce is a matrix that compares the output of all mixers and computes the correlation between them.</p>
<p>Let’s implement the algorithm:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> model_correlation.py

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">SimpleNamespace</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">lightwood.ensemble</span> <span class="kn">import</span> <span class="n">BestOf</span>
<span class="kn">from</span> <span class="nn">lightwood.analysis</span> <span class="kn">import</span> <span class="n">BaseAnalysisBlock</span>


<span class="k">class</span> <span class="nc">ModelCorrelationHeatmap</span><span class="p">(</span><span class="n">BaseAnalysisBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deps</span><span class="o">=</span><span class="nb">tuple</span><span class="p">()):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">deps</span><span class="o">=</span><span class="n">deps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">analyze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]:</span>
        <span class="n">ns</span> <span class="o">=</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># only triggered with the right type of ensemble</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ns</span><span class="o">.</span><span class="n">predictor</span><span class="p">,</span> <span class="n">BestOf</span><span class="p">):</span>

            <span class="c1"># store prediction from every mixer</span>
            <span class="n">all_predictions</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">mixer</span> <span class="ow">in</span> <span class="n">ns</span><span class="o">.</span><span class="n">predictor</span><span class="o">.</span><span class="n">mixers</span><span class="p">:</span>
                <span class="n">predictions</span> <span class="o">=</span> <span class="n">mixer</span><span class="p">(</span><span class="n">ns</span><span class="o">.</span><span class="n">encoded_val_data</span><span class="p">)[</span><span class="s1">&#39;prediction&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># retrieve np.ndarray from the returned pd.DataFrame</span>
                <span class="n">all_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>  <span class="c1"># flatten and cast labels to int</span>

            <span class="c1"># calculate correlation matrix</span>
            <span class="n">corrs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">all_predictions</span><span class="p">))</span>

            <span class="c1"># save inside `info` object</span>
            <span class="n">info</span><span class="p">[</span><span class="s1">&#39;mixer_correlation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">corrs</span>

        <span class="k">return</span> <span class="n">info</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Writing model_correlation.py
</pre></div></div>
</div>
<p>Notice the use of <code class="docutils literal notranslate"><span class="pre">SimpleNamespace</span></code> for dot notation accessors.</p>
<p>The procedure above is fairly straightforward, as we leverage numpy’s <code class="docutils literal notranslate"><span class="pre">corrcoef()</span></code> function to generate the matrix.</p>
<p>Finally, it is very important to add the output to <code class="docutils literal notranslate"><span class="pre">info</span></code> so that it is saved inside the actual predictor object.</p>
</div>
<div class="section" id="Step-3:-Exposing-the-block-to-Lightwood">
<h2>Step 3: Exposing the block to Lightwood<a class="headerlink" href="#Step-3:-Exposing-the-block-to-Lightwood" title="Permalink to this headline">¶</a></h2>
<p>To use this in an arbitrary script, we need to add the above class (and all necessary imports) to a <code class="docutils literal notranslate"><span class="pre">.py</span></code> file inside one of the following directories:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">~/lightwood_modules</span></code> (where <code class="docutils literal notranslate"><span class="pre">~</span></code> is your home directory, e.g. <code class="docutils literal notranslate"><span class="pre">/Users/username/</span></code> for macOS and <code class="docutils literal notranslate"><span class="pre">/home/username/</span></code> for linux</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/etc/lightwood_modules</span></code></p></li>
</ul>
<p>Lightwood will scan these directories and import any class so that they can be found and used by the <code class="docutils literal notranslate"><span class="pre">JsonAI</span></code> code generating module.</p>
<p><strong>To continue, please save the code cell above as ``model_correlation.py`` in one of the indicated directories.</strong></p>
</div>
<div class="section" id="Step-4:-Final-test-run">
<h2>Step 4: Final test run<a class="headerlink" href="#Step-4:-Final-test-run" title="Permalink to this headline">¶</a></h2>
<p>Ok! Everything looks set to try out our custom block. Let’s generate a predictor for <a class="reference external" href="https://github.com/mindsdb/lightwood/blob/stable/tests/data/hdi.csv">this</a> sample dataset, and see whether our new insights are any good.</p>
<p>First, it is important to add our <code class="docutils literal notranslate"><span class="pre">ModelCorrelationHeatmap</span></code> to the <code class="docutils literal notranslate"><span class="pre">analysis_blocks</span></code> attribute of the Json AI object that will generate your predictor code.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightwood.api.high_level</span> <span class="kn">import</span> <span class="n">ProblemDefinition</span><span class="p">,</span> <span class="n">json_ai_from_problem</span><span class="p">,</span> <span class="n">load_custom_module</span>

<span class="c1"># First, load the custom module we wrote</span>
<span class="n">load_custom_module</span><span class="p">(</span><span class="s1">&#39;model_correlation.py&#39;</span><span class="p">)</span>

<span class="c1"># read dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/mindsdb/lightwood/stable/tests/data/hdi.csv&#39;</span><span class="p">)</span>

<span class="c1"># define the predictive task</span>
<span class="n">pdef</span> <span class="o">=</span> <span class="n">ProblemDefinition</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="s1">&#39;Development Index&#39;</span><span class="p">,</span>         <span class="c1"># column you want to predict</span>
    <span class="s1">&#39;time_aim&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
<span class="p">})</span>

<span class="c1"># generate the Json AI intermediate representation from the data and its corresponding settings</span>
<span class="n">json_ai</span> <span class="o">=</span> <span class="n">json_ai_from_problem</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">problem_definition</span><span class="o">=</span><span class="n">pdef</span><span class="p">)</span>

<span class="c1"># add the custom list of analysis blocks; in this case, composed of a single block</span>
<span class="n">json_ai</span><span class="o">.</span><span class="n">analysis_blocks</span> <span class="o">=</span> <span class="p">[{</span>
    <span class="s1">&#39;module&#39;</span><span class="p">:</span> <span class="s1">&#39;model_correlation.ModelCorrelationHeatmap&#39;</span><span class="p">,</span>
    <span class="s1">&#39;args&#39;</span><span class="p">:</span> <span class="p">{}</span>
<span class="p">}]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:type_infer-2216:Analyzing a sample of 222</span>
<span class="ansi-green-fg">INFO:type_infer-2216:from a total population of 225, this is equivalent to 98.7% of your data.</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Infering type for: Population</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Column Population has data type integer</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Infering type for: Area (sq. mi.)</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Column Area (sq. mi.) has data type integer</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Infering type for: Pop. Density </span>
<span class="ansi-green-fg">INFO:type_infer-2216:Column Pop. Density  has data type float</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Infering type for: GDP ($ per capita)</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Column GDP ($ per capita) has data type integer</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Infering type for: Literacy (%)</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Column Literacy (%) has data type float</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Infering type for: Infant mortality </span>
<span class="ansi-green-fg">INFO:type_infer-2216:Column Infant mortality  has data type float</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Infering type for: Development Index</span>
<span class="ansi-green-fg">INFO:type_infer-2216:Column Development Index has data type categorical</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Starting statistical analysis</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Finished statistical analysis</span>
</pre></div></div>
</div>
<p>We can take a look at the respective Json AI key just to confirm our newly added analysis block is in there:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">json_ai</span><span class="o">.</span><span class="n">analysis_blocks</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[{&#39;module&#39;: &#39;model_correlation.ModelCorrelationHeatmap&#39;, &#39;args&#39;: {}}]
</pre></div></div>
</div>
<p>Now we are ready to create a predictor from this Json AI, and subsequently train it:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightwood.api.high_level</span> <span class="kn">import</span> <span class="n">code_from_json_ai</span><span class="p">,</span> <span class="n">predictor_from_code</span>

<span class="n">code</span> <span class="o">=</span> <span class="n">code_from_json_ai</span><span class="p">(</span><span class="n">json_ai</span><span class="p">)</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">predictor_from_code</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>

<span class="n">predictor</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Unable to import black formatter, predictor code might be a bit ugly.</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:[Learn phase 1/8] - Statistical analysis</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Starting statistical analysis</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Finished statistical analysis</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `analyze_data` runtime: 0.02 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:[Learn phase 2/8] - Data preprocessing</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Cleaning the data</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `preprocess` runtime: 0.01 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:[Learn phase 3/8] - Data splitting</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Splitting the data into train/test</span>
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:131: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  train_st = train_st.append(df[:train_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:132: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  dev_st = dev_st.append(df[train_cutoff:dev_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:133: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  test_st = test_st.append(df[dev_cutoff:test_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:131: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  train_st = train_st.append(df[:train_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:132: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  dev_st = dev_st.append(df[train_cutoff:dev_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:133: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  test_st = test_st.append(df[dev_cutoff:test_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:131: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  train_st = train_st.append(df[:train_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:132: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  dev_st = dev_st.append(df[train_cutoff:dev_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:133: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  test_st = test_st.append(df[dev_cutoff:test_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:131: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  train_st = train_st.append(df[:train_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:132: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  dev_st = dev_st.append(df[train_cutoff:dev_cutoff])
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/dataprep_ml/splitters.py:133: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  test_st = test_st.append(df[dev_cutoff:test_cutoff])
<span class="ansi-white-fg">DEBUG:lightwood-2216: `split` runtime: 0.03 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:[Learn phase 4/8] - Preparing encoders</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Encoding UNKNOWN categories as index 0</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `prepare` runtime: 0.01 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:[Learn phase 5/8] - Feature generation</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Featurizing the data</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `featurize` runtime: 0.0 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:[Learn phase 6/8] - Mixer training</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Training the mixers</span>
<span class="ansi-yellow-fg">WARNING:lightwood-2216:XGBoost running on CPU</span>
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(&#34;torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.&#34;)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[14:03:15] WARNING: ../src/learner.cc:339: No visible GPU is found, setting `gpu_id` to -1
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/pytorch_ranger/ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:
        addcmul_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
        addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)
  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
<span class="ansi-green-fg">INFO:lightwood-2216:Loss of 2.0416805744171143 with learning rate 0.0001</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss of 1.8292739391326904 with learning rate 0.00014</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss of 1.9874004125595093 with learning rate 0.00019599999999999997</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Found learning rate of: 0.00014</span>
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(&#34;torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.&#34;)
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 1: 1.5811586380004883</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 2: 1.6478627920150757</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 3: 1.6348984241485596</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 4: 1.6213680505752563</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 5: 1.606968641281128</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 6: 1.5919418334960938</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 7: 1.5534838438034058</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 8: 1.5364340543746948</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 9: 1.519599437713623</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 10: 1.5030533075332642</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 11: 1.4867080450057983</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 12: 1.4707883596420288</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 13: 1.4331674575805664</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 14: 1.418662190437317</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 15: 1.4047675132751465</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 16: 1.3915122747421265</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 17: 1.3787592649459839</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 18: 1.366633653640747</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 19: 1.3388729095458984</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 20: 1.328777551651001</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 21: 1.319250226020813</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 22: 1.3103091716766357</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 23: 1.301840901374817</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 24: 1.2938703298568726</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 25: 1.275942087173462</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 26: 1.2696194648742676</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 27: 1.263667345046997</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 28: 1.2580868005752563</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 29: 1.2528012990951538</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 30: 1.2477504014968872</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 31: 1.2361828088760376</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 32: 1.2319444417953491</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 33: 1.2278447151184082</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 34: 1.2238948345184326</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 35: 1.2200753688812256</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 36: 1.2162835597991943</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 37: 1.2072482109069824</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 38: 1.2037603855133057</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 39: 1.2003237009048462</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 40: 1.1969619989395142</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 41: 1.193686842918396</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 42: 1.1903716325759888</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 43: 1.1823793649673462</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 44: 1.1792428493499756</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 45: 1.1761399507522583</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 46: 1.1730939149856567</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 47: 1.170122742652893</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 48: 1.1670866012573242</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 49: 1.1597480773925781</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 50: 1.1568483114242554</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 51: 1.1539745330810547</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 52: 1.1511521339416504</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 53: 1.148398756980896</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 54: 1.1455658674240112</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 55: 1.138717770576477</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 56: 1.1360077857971191</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 57: 1.133321762084961</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 58: 1.1306843757629395</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 59: 1.1281135082244873</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 60: 1.1254539489746094</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 61: 1.1190311908721924</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 62: 1.1164851188659668</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 63: 1.1139603853225708</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 64: 1.1114822626113892</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 65: 1.109068512916565</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 66: 1.106554627418518</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 67: 1.1004847288131714</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 68: 1.0980749130249023</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 69: 1.0956851243972778</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 70: 1.0933388471603394</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 71: 1.0910571813583374</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 72: 1.0886691808700562</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 73: 1.082910180091858</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 74: 1.080618143081665</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 75: 1.0783448219299316</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 76: 1.0761151313781738</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 77: 1.0739494562149048</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 78: 1.0716668367385864</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 79: 1.066169023513794</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 80: 1.063974380493164</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 81: 1.0617973804473877</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 82: 1.0596634149551392</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 83: 1.0575934648513794</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 84: 1.0553996562957764</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 85: 1.0501261949539185</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 86: 1.0480172634124756</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 87: 1.0459239482879639</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 88: 1.043873906135559</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 89: 1.0418872833251953</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 90: 1.0397709608078003</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 91: 1.034684419631958</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 92: 1.032646894454956</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 93: 1.0306257009506226</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 94: 1.0286478996276855</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 95: 1.0267351865768433</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 96: 1.0246872901916504</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 97: 1.0197736024856567</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 98: 1.0178049802780151</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 99: 1.0158514976501465</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 100: 1.013943076133728</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 101: 1.012105107307434</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 102: 1.0101375579833984</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 103: 1.0054210424423218</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 104: 1.0035293102264404</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 105: 1.0016531944274902</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 106: 0.999819815158844</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 107: 0.9980561137199402</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 108: 0.996149480342865</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 109: 0.9915881156921387</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 110: 0.9897538423538208</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 111: 0.9879361391067505</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 112: 0.9861626625061035</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 113: 0.9844597578048706</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 114: 0.9826090335845947</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 115: 0.9781792759895325</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 116: 0.9763942360877991</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 117: 0.9746262431144714</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 118: 0.9729020595550537</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 119: 0.9712491035461426</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 120: 0.9694430232048035</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 121: 0.9651228785514832</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 122: 0.9633989334106445</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 123: 0.9617007970809937</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 124: 0.9600439071655273</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 125: 0.9584614038467407</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 126: 0.9567253589630127</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 127: 0.9525789618492126</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 128: 0.9509056806564331</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 129: 0.949250340461731</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 130: 0.9476391077041626</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 131: 0.9461030960083008</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 132: 0.9444094896316528</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 133: 0.9403712153434753</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 134: 0.938744068145752</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 135: 0.9371326565742493</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 136: 0.935562014579773</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 137: 0.9340692758560181</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 138: 0.9324181079864502</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 139: 0.9284619092941284</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 140: 0.9268574118614197</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 141: 0.925269365310669</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 142: 0.9237228631973267</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 143: 0.922257125377655</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 144: 0.920630693435669</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 145: 0.9167453050613403</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 146: 0.9151697754859924</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 147: 0.9136118292808533</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 148: 0.9120946526527405</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 149: 0.9106616973876953</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 150: 0.9090710282325745</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 151: 0.9052843451499939</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 152: 0.9037502408027649</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 153: 0.9022330641746521</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 154: 0.9007584452629089</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 155: 0.8993725180625916</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 156: 0.8978197574615479</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 157: 0.8941053152084351</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 158: 0.892594039440155</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 159: 0.8911020159721375</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 160: 0.8896512985229492</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 161: 0.8882890939712524</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 162: 0.8867582678794861</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 163: 0.8831132650375366</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 164: 0.8816302418708801</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 165: 0.8801626563072205</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 166: 0.878736674785614</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 167: 0.877403736114502</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 168: 0.8758909702301025</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 169: 0.8722763061523438</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 170: 0.8708068132400513</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 171: 0.8693538308143616</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 172: 0.8679428100585938</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 173: 0.8666291832923889</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 174: 0.8651421070098877</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 175: 0.8616037368774414</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 176: 0.8601579666137695</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 177: 0.8587303161621094</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 178: 0.8573461771011353</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 179: 0.8560625314712524</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 180: 0.8546043634414673</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 181: 0.851116955280304</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 182: 0.8496885299682617</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 183: 0.8482735753059387</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 184: 0.8469024896621704</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 185: 0.8456344604492188</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 186: 0.844188928604126</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 187: 0.84076327085495</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 188: 0.8393596410751343</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 189: 0.8379706740379333</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 190: 0.8366269469261169</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 191: 0.8353926539421082</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 192: 0.8339842557907104</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 193: 0.8306221961975098</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 194: 0.8292466402053833</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 195: 0.8278875350952148</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 196: 0.8265740275382996</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 197: 0.8253715634346008</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 198: 0.8239877820014954</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 199: 0.820676326751709</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 200: 0.8193195462226868</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 201: 0.8179788589477539</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 202: 0.8166859745979309</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 203: 0.815505862236023</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 204: 0.8141486048698425</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 205: 0.8109045028686523</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 206: 0.8095766305923462</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 207: 0.8082641959190369</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 208: 0.806999146938324</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 209: 0.8058490753173828</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 210: 0.8045154213905334</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 211: 0.8013198971748352</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 212: 0.8000099062919617</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 213: 0.798710286617279</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 214: 0.7974594235420227</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 215: 0.7963274717330933</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 216: 0.7950125932693481</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 217: 0.7918611764907837</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 218: 0.7905637621879578</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 219: 0.7892813086509705</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 220: 0.7880480289459229</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 221: 0.7869349718093872</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 222: 0.785637378692627</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 223: 0.7825192213058472</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 224: 0.7812315225601196</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 225: 0.7799572944641113</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 226: 0.7787392139434814</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 227: 0.7776491641998291</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 228: 0.7763760089874268</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 229: 0.7733224630355835</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 230: 0.7720646858215332</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 231: 0.7708194851875305</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 232: 0.7696265578269958</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 233: 0.7685608863830566</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 234: 0.7673083543777466</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 235: 0.7642987370491028</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 236: 0.7630563378334045</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 237: 0.7618279457092285</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 238: 0.7606557011604309</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 239: 0.7596102356910706</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 240: 0.758376955986023</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 241: 0.7554044723510742</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 242: 0.7541752457618713</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 243: 0.7529587745666504</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 244: 0.7517977952957153</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 245: 0.7507689595222473</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 246: 0.7495521903038025</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 247: 0.7466228604316711</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 248: 0.7454093098640442</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 249: 0.7442083358764648</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 250: 0.7430630922317505</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 251: 0.742050051689148</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 252: 0.7408469915390015</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 253: 0.7379463911056519</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 254: 0.7367435693740845</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 255: 0.7355575561523438</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 256: 0.734430193901062</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 257: 0.7334378361701965</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 258: 0.7322563529014587</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 259: 0.7294049263000488</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 260: 0.728220522403717</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 261: 0.7270462512969971</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 262: 0.7259324193000793</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 263: 0.7249569296836853</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 264: 0.7237886786460876</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 265: 0.7209699749946594</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 266: 0.7197951674461365</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 267: 0.7186312675476074</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 268: 0.7175272703170776</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 269: 0.7165635228157043</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 270: 0.7154060006141663</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 271: 0.712605357170105</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 272: 0.71143639087677</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 273: 0.710282564163208</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 274: 0.7091926336288452</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 275: 0.7082464694976807</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 276: 0.7071049213409424</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 277: 0.704345166683197</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 278: 0.703192412853241</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 279: 0.7020503282546997</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 280: 0.7009741067886353</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 281: 0.7000429630279541</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 282: 0.6989173293113708</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 283: 0.6961947083473206</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 284: 0.695054829120636</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 285: 0.693927526473999</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 286: 0.6928672194480896</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 287: 0.6919549703598022</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 288: 0.69084632396698</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 289: 0.688159167766571</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 290: 0.6870335340499878</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 291: 0.6859201788902283</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 292: 0.6848765015602112</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 293: 0.6839823126792908</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 294: 0.6828893423080444</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 295: 0.6802409887313843</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 296: 0.6791319251060486</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 297: 0.6780346632003784</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 298: 0.6770076155662537</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 299: 0.6761307716369629</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 300: 0.6750547289848328</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 301: 0.6724516153335571</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 302: 0.6713587641716003</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 303: 0.6702784895896912</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 304: 0.6692715883255005</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 305: 0.6684154868125916</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 306: 0.6673574447631836</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 307: 0.6647851467132568</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 308: 0.663699746131897</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 309: 0.6626246571540833</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 310: 0.6616238355636597</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 311: 0.6607753038406372</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 312: 0.6597260236740112</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 313: 0.6571747660636902</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 314: 0.6561022400856018</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 315: 0.6550400257110596</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 316: 0.654054582118988</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 317: 0.6532227396965027</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 318: 0.6521891355514526</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 319: 0.6496785879135132</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 320: 0.6486220359802246</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 321: 0.6475772261619568</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 322: 0.6466100811958313</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 323: 0.6457949280738831</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 324: 0.6447750329971313</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 325: 0.6422935128211975</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 326: 0.6412427425384521</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 327: 0.6401992440223694</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 328: 0.6392336487770081</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 329: 0.6384223103523254</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 330: 0.6374073624610901</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 331: 0.6349397301673889</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 332: 0.6338951587677002</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 333: 0.6328575015068054</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 334: 0.6318976879119873</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 335: 0.6310930848121643</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 336: 0.630081832408905</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 337: 0.6276143789291382</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 338: 0.6265742182731628</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 339: 0.6255477666854858</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 340: 0.6246057748794556</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 341: 0.6238208413124084</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 342: 0.622830867767334</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 343: 0.6204220652580261</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 344: 0.6194019317626953</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 345: 0.6184098720550537</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 346: 0.6175112724304199</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 347: 0.6167705059051514</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 348: 0.6158262491226196</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 349: 0.6135278940200806</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 350: 0.6125521659851074</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 351: 0.6115869283676147</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 352: 0.6107121706008911</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 353: 0.6099924445152283</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 354: 0.6090706586837769</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 355: 0.6068308353424072</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 356: 0.6058895587921143</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 357: 0.604961097240448</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 358: 0.6041210293769836</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 359: 0.603434681892395</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 360: 0.602545976638794</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 361: 0.6003788113594055</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 362: 0.599454402923584</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 363: 0.5985423922538757</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 364: 0.5977196097373962</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 365: 0.5970555543899536</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 366: 0.5961887836456299</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 367: 0.5940759181976318</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 368: 0.5931720733642578</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 369: 0.5922737717628479</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 370: 0.5914672017097473</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 371: 0.5908126831054688</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 372: 0.5899550914764404</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 373: 0.5878630876541138</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 374: 0.5869656801223755</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 375: 0.586074948310852</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 376: 0.5852754712104797</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 377: 0.5846306085586548</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 378: 0.5837823748588562</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 379: 0.5817071199417114</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 380: 0.5808115601539612</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 381: 0.5799278616905212</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 382: 0.5791399478912354</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 383: 0.5785082578659058</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 384: 0.5776743292808533</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 385: 0.5756447911262512</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 386: 0.5747730135917664</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 387: 0.5739051699638367</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 388: 0.5731337070465088</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 389: 0.572521984577179</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 390: 0.5717072486877441</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 391: 0.5697109699249268</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 392: 0.5688461661338806</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 393: 0.567988932132721</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 394: 0.567231297492981</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 395: 0.566628634929657</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 396: 0.5658200979232788</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 397: 0.5638426542282104</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 398: 0.5629952549934387</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 399: 0.5621517300605774</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 400: 0.5614095330238342</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 401: 0.5608246326446533</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 402: 0.560034990310669</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 403: 0.558106005191803</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 404: 0.5572667717933655</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 405: 0.5564338564872742</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 406: 0.5557036995887756</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 407: 0.555128812789917</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 408: 0.5543506741523743</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 409: 0.5524493455886841</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 410: 0.5516209602355957</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 411: 0.550791323184967</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 412: 0.5500703454017639</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 413: 0.5495014190673828</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 414: 0.5487274527549744</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 415: 0.5468266606330872</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 416: 0.5459935665130615</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 417: 0.5451645851135254</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 418: 0.5444376468658447</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 419: 0.5438663363456726</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 420: 0.5430910587310791</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 421: 0.5411869883537292</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 422: 0.5403589010238647</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 423: 0.5395379066467285</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 424: 0.5388223528862</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 425: 0.5382633209228516</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 426: 0.537501335144043</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 427: 0.5356349349021912</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 428: 0.5348190665245056</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 429: 0.5340111255645752</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 430: 0.5333110690116882</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 431: 0.5327656865119934</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 432: 0.5320170521736145</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 433: 0.5301740169525146</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 434: 0.5293665528297424</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 435: 0.5285669565200806</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 436: 0.5278795957565308</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 437: 0.5273411273956299</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 438: 0.5266006588935852</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 439: 0.5247769355773926</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 440: 0.523978054523468</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 441: 0.5231837630271912</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 442: 0.5225073099136353</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 443: 0.5219779014587402</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 444: 0.5212475061416626</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 445: 0.519446849822998</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 446: 0.5186613202095032</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 447: 0.5178760886192322</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 448: 0.5172038078308105</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 449: 0.5166839957237244</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 450: 0.5159632563591003</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 451: 0.5141810178756714</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 452: 0.5133962631225586</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 453: 0.5126131176948547</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 454: 0.5119463205337524</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 455: 0.511427104473114</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 456: 0.5107095241546631</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 457: 0.5089320540428162</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 458: 0.5081538558006287</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 459: 0.5073782205581665</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 460: 0.5067210793495178</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 461: 0.5062054991722107</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 462: 0.5054914355278015</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 463: 0.5037287473678589</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 464: 0.5029617547988892</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 465: 0.5021977424621582</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 466: 0.5015439391136169</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 467: 0.5010324120521545</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 468: 0.5003252029418945</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 469: 0.49857351183891296</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 470: 0.49779999256134033</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 471: 0.49703267216682434</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 472: 0.4963838756084442</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 473: 0.495875746011734</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 474: 0.4951745867729187</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 475: 0.4934527575969696</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 476: 0.4926978349685669</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 477: 0.4919407069683075</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 478: 0.4913072884082794</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 479: 0.4908131957054138</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 480: 0.49012669920921326</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 481: 0.4884376525878906</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 482: 0.48770132660865784</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 483: 0.48696547746658325</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 484: 0.48635125160217285</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 485: 0.4858754575252533</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 486: 0.4852069616317749</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 487: 0.48355355858802795</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 488: 0.48282065987586975</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 489: 0.48209133744239807</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 490: 0.4814847707748413</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 491: 0.4810087978839874</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 492: 0.48034030199050903</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 493: 0.4786792993545532</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 494: 0.47794508934020996</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 495: 0.47720327973365784</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 496: 0.4765933156013489</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 497: 0.47611597180366516</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 498: 0.47544780373573303</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 499: 0.4737856388092041</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 500: 0.47305387258529663</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 501: 0.4723232686519623</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 502: 0.4717152416706085</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 503: 0.47123703360557556</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 504: 0.4705696702003479</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 505: 0.46891477704048157</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 506: 0.4681890308856964</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 507: 0.4674580991268158</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 508: 0.4668571352958679</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 509: 0.4663849174976349</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 510: 0.46572384238243103</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 511: 0.464083194732666</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 512: 0.46336326003074646</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 513: 0.46264126896858215</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 514: 0.4620561897754669</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 515: 0.4615927040576935</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 516: 0.4609401822090149</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 517: 0.45931336283683777</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 518: 0.45859429240226746</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 519: 0.4578750431537628</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 520: 0.45728784799575806</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 521: 0.456828236579895</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 522: 0.45618465542793274</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 523: 0.4545990526676178</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 524: 0.4538951516151428</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 525: 0.45319896936416626</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 526: 0.45262885093688965</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 527: 0.4521842300891876</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 528: 0.4515542984008789</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 529: 0.44998273253440857</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 530: 0.4492848515510559</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 531: 0.44858840107917786</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 532: 0.44802790880203247</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 533: 0.44758835434913635</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 534: 0.4469626545906067</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 535: 0.4454008638858795</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 536: 0.4447055757045746</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 537: 0.4440203607082367</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 538: 0.44346877932548523</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 539: 0.44303786754608154</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 540: 0.4424269497394562</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 541: 0.44090017676353455</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 542: 0.4402161240577698</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 543: 0.43953725695610046</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 544: 0.4389885663986206</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 545: 0.43856045603752136</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 546: 0.4379488229751587</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 547: 0.43642276525497437</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 548: 0.4357452988624573</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 549: 0.4350759983062744</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 550: 0.43454238772392273</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 551: 0.4341264069080353</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 552: 0.4335322976112366</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 553: 0.43203794956207275</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 554: 0.43137127161026</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 555: 0.4307125210762024</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 556: 0.43019187450408936</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 557: 0.4297819137573242</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 558: 0.42919474840164185</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 559: 0.4277138113975525</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 560: 0.42705458402633667</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 561: 0.4263990521430969</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 562: 0.4258926510810852</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 563: 0.4254929721355438</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 564: 0.42492157220840454</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 565: 0.4234866797924042</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 566: 0.4228467345237732</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 567: 0.42220479249954224</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 568: 0.42170530557632446</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 569: 0.4213161766529083</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 570: 0.4207512438297272</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 571: 0.41933679580688477</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 572: 0.41870933771133423</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 573: 0.418087363243103</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 574: 0.41760966181755066</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 575: 0.4172336161136627</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 576: 0.4166868031024933</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 577: 0.4153108596801758</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 578: 0.41469043493270874</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 579: 0.41407328844070435</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 580: 0.41361236572265625</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 581: 0.41324567794799805</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 582: 0.4127092957496643</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 583: 0.41135331988334656</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 584: 0.4107401967048645</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 585: 0.41013699769973755</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 586: 0.4096837043762207</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 587: 0.40932705998420715</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 588: 0.40880563855171204</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 589: 0.4074767827987671</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 590: 0.40686336159706116</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 591: 0.4062609076499939</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 592: 0.40580931305885315</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 593: 0.4054555594921112</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 594: 0.4049341082572937</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 595: 0.4036155939102173</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 596: 0.40301603078842163</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 597: 0.40243107080459595</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 598: 0.4019860625267029</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 599: 0.4016270637512207</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 600: 0.40111154317855835</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 601: 0.3997997045516968</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 602: 0.39919689297676086</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 603: 0.39860594272613525</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 604: 0.39816173911094666</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 605: 0.39781349897384644</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 606: 0.3973061442375183</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 607: 0.3960060775279999</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 608: 0.39541515707969666</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 609: 0.3948219418525696</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 610: 0.3943871557712555</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 611: 0.3940356969833374</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 612: 0.39352530241012573</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 613: 0.3922238051891327</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 614: 0.39163413643836975</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 615: 0.39105042815208435</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 616: 0.39062368869781494</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 617: 0.3902781009674072</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 618: 0.38977813720703125</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 619: 0.38851118087768555</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 620: 0.387940376996994</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 621: 0.3873691260814667</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 622: 0.3869527578353882</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 623: 0.38661324977874756</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 624: 0.3861270546913147</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 625: 0.38487985730171204</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 626: 0.38429877161979675</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 627: 0.38372603058815</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 628: 0.3833182454109192</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 629: 0.38298216462135315</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 630: 0.3824997842311859</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 631: 0.38126182556152344</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 632: 0.3806990087032318</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 633: 0.3801456391811371</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 634: 0.3797451853752136</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 635: 0.37941834330558777</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 636: 0.3789506256580353</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 637: 0.3777407705783844</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 638: 0.37718257308006287</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 639: 0.376629501581192</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 640: 0.3762427270412445</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 641: 0.37592360377311707</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 642: 0.3754616677761078</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 643: 0.3742620348930359</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 644: 0.3737102150917053</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 645: 0.3731641471385956</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 646: 0.37278735637664795</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 647: 0.37247174978256226</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 648: 0.3720162808895111</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 649: 0.37083274126052856</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 650: 0.37029600143432617</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 651: 0.3697642982006073</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 652: 0.36939504742622375</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 653: 0.36909112334251404</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 654: 0.36865049600601196</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 655: 0.36749035120010376</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 656: 0.3669526278972626</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 657: 0.36642298102378845</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 658: 0.36606407165527344</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 659: 0.3657618761062622</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 660: 0.3653278648853302</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 661: 0.3641931712627411</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 662: 0.3636700212955475</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 663: 0.3631454110145569</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 664: 0.36278387904167175</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 665: 0.36248481273651123</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 666: 0.36205196380615234</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 667: 0.36091697216033936</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 668: 0.36040058732032776</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 669: 0.3598894476890564</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 670: 0.3595430850982666</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 671: 0.35925203561782837</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 672: 0.3588293492794037</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 673: 0.3577122688293457</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 674: 0.35720306634902954</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 675: 0.3566930890083313</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 676: 0.35635650157928467</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 677: 0.35606685280799866</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 678: 0.3556496202945709</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 679: 0.3545461595058441</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 680: 0.3540380001068115</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 681: 0.35353630781173706</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 682: 0.35320326685905457</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 683: 0.35291922092437744</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 684: 0.3525114059448242</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 685: 0.35142678022384644</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 686: 0.35091572999954224</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 687: 0.3504124581813812</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 688: 0.3500782549381256</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 689: 0.3497951924800873</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 690: 0.34939494729042053</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 691: 0.3483375012874603</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 692: 0.3478415906429291</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 693: 0.34735599160194397</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 694: 0.3470439612865448</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 695: 0.3467714786529541</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 696: 0.3463802933692932</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 697: 0.3453296720981598</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 698: 0.3448297083377838</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 699: 0.34434592723846436</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 700: 0.34403303265571594</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 701: 0.3437635898590088</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 702: 0.34337949752807617</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 703: 0.34235164523124695</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 704: 0.3418674170970917</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 705: 0.3413851857185364</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 706: 0.34107834100723267</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 707: 0.34080931544303894</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 708: 0.34042513370513916</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 709: 0.33938923478126526</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 710: 0.33890047669410706</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 711: 0.3384246528148651</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 712: 0.338121235370636</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 713: 0.3378627300262451</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 714: 0.3374917209148407</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 715: 0.3364810347557068</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 716: 0.3360036313533783</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 717: 0.33553487062454224</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 718: 0.3352438509464264</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 719: 0.33497726917266846</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 720: 0.3345996141433716</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 721: 0.3335878849029541</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 722: 0.3331127464771271</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 723: 0.33264872431755066</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 724: 0.3323664367198944</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 725: 0.33211761713027954</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 726: 0.3317626714706421</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 727: 0.3308013677597046</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 728: 0.330332487821579</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 729: 0.3298781216144562</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 730: 0.3295995891094208</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 731: 0.3293435573577881</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 732: 0.3289891481399536</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 733: 0.3280251622200012</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 734: 0.32757312059402466</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 735: 0.3271225690841675</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 736: 0.326850950717926</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 737: 0.32660573720932007</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 738: 0.32625812292099</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 739: 0.32530778646469116</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 740: 0.3248522877693176</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 741: 0.3244095742702484</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 742: 0.3241477906703949</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 743: 0.32390984892845154</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 744: 0.32357117533683777</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 745: 0.3226418197154999</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 746: 0.3221958577632904</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 747: 0.321757048368454</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 748: 0.32150042057037354</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 749: 0.3212651014328003</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 750: 0.3209352493286133</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 751: 0.3200185298919678</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 752: 0.3195774555206299</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 753: 0.3191455900669098</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 754: 0.3188944160938263</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 755: 0.31865665316581726</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 756: 0.31833216547966003</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 757: 0.317430704832077</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 758: 0.3169941008090973</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 759: 0.31656625866889954</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 760: 0.31631606817245483</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 761: 0.3160841166973114</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 762: 0.31576529145240784</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 763: 0.3148770034313202</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 764: 0.3144480586051941</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 765: 0.3140336275100708</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 766: 0.3138042092323303</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 767: 0.3135824203491211</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 768: 0.31327491998672485</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 769: 0.3124161660671234</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 770: 0.3119925856590271</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 771: 0.3115786612033844</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 772: 0.311350017786026</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 773: 0.31113365292549133</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 774: 0.3108343183994293</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 775: 0.30999308824539185</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 776: 0.3095853328704834</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 777: 0.3091929256916046</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 778: 0.3089714050292969</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 779: 0.3087548613548279</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 780: 0.3084568381309509</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 781: 0.3076104521751404</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 782: 0.3072030544281006</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 783: 0.3068086504936218</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 784: 0.3065996766090393</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 785: 0.30638986825942993</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 786: 0.30610391497612</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 787: 0.30527937412261963</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 788: 0.3048742711544037</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 789: 0.30447956919670105</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 790: 0.3042648136615753</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 791: 0.3040522634983063</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 792: 0.30377140641212463</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 793: 0.30297181010246277</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 794: 0.3025755286216736</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 795: 0.3021929860115051</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 796: 0.30199918150901794</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 797: 0.301800400018692</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 798: 0.30153176188468933</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 799: 0.3007533550262451</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 800: 0.30037227272987366</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 801: 0.2999999523162842</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 802: 0.2998107969760895</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 803: 0.29961714148521423</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 804: 0.2993537187576294</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 805: 0.2985948622226715</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 806: 0.29822584986686707</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 807: 0.2978682219982147</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 808: 0.2976856529712677</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 809: 0.29749584197998047</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 810: 0.29724329710006714</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 811: 0.29650261998176575</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 812: 0.29613736271858215</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 813: 0.29578378796577454</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 814: 0.29561176896095276</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 815: 0.2954300343990326</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 816: 0.29518622159957886</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 817: 0.29445552825927734</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 818: 0.2941022217273712</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 819: 0.2937605679035187</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 820: 0.29359155893325806</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 821: 0.2934131920337677</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 822: 0.29317381978034973</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 823: 0.292451947927475</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 824: 0.29210034012794495</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 825: 0.29175421595573425</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 826: 0.2915888726711273</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 827: 0.29140621423721313</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 828: 0.2911686301231384</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 829: 0.290459543466568</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 830: 0.2901204228401184</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 831: 0.28977832198143005</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 832: 0.28961601853370667</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 833: 0.2894386351108551</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 834: 0.28921079635620117</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 835: 0.2885006070137024</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 836: 0.28815698623657227</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 837: 0.28782355785369873</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 838: 0.28766027092933655</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 839: 0.28748029470443726</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 840: 0.28725194931030273</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 841: 0.28654876351356506</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 842: 0.2862073481082916</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 843: 0.28587082028388977</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 844: 0.28571173548698425</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 845: 0.2855300009250641</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 846: 0.28530165553092957</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 847: 0.2845958471298218</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 848: 0.28425467014312744</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 849: 0.28391435742378235</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 850: 0.283752977848053</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 851: 0.28357014060020447</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 852: 0.28334277868270874</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 853: 0.28263163566589355</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 854: 0.28228282928466797</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 855: 0.28194698691368103</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 856: 0.2817786931991577</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 857: 0.28159254789352417</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 858: 0.2813641428947449</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 859: 0.2806667983531952</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 860: 0.2803279459476471</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 861: 0.27999821305274963</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 862: 0.2798500061035156</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 863: 0.27967405319213867</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 864: 0.2794584631919861</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 865: 0.2787800133228302</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 866: 0.27844613790512085</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 867: 0.2781190872192383</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 868: 0.27797189354896545</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 869: 0.2777943015098572</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 870: 0.2775789797306061</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 871: 0.2768968939781189</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 872: 0.27657249569892883</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 873: 0.27625012397766113</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 874: 0.27609920501708984</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 875: 0.2759155035018921</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 876: 0.27569907903671265</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 877: 0.2750225365161896</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 878: 0.27468544244766235</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 879: 0.2743622958660126</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 880: 0.2742200791835785</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 881: 0.2740408480167389</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 882: 0.2738279402256012</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 883: 0.27316224575042725</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 884: 0.27283021807670593</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 885: 0.2725027799606323</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 886: 0.27235910296440125</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 887: 0.27218055725097656</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 888: 0.2719719111919403</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 889: 0.2713078558444977</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 890: 0.2709701955318451</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 891: 0.27065059542655945</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 892: 0.2705080807209015</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 893: 0.2703239619731903</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 894: 0.27011242508888245</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 895: 0.2694456875324249</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 896: 0.2691183388233185</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 897: 0.2687987685203552</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 898: 0.26865315437316895</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 899: 0.26847052574157715</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 900: 0.26826274394989014</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 901: 0.26759493350982666</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 902: 0.26726529002189636</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 903: 0.26694637537002563</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 904: 0.2668039798736572</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 905: 0.266619473695755</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 906: 0.26641377806663513</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 907: 0.2657501697540283</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 908: 0.26541656255722046</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 909: 0.26509448885917664</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 910: 0.2649480402469635</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 911: 0.26476240158081055</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 912: 0.2645536959171295</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 913: 0.26388072967529297</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 914: 0.2635432481765747</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 915: 0.2632201910018921</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 916: 0.2630787491798401</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 917: 0.2628880441188812</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 918: 0.2626815140247345</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 919: 0.2620163559913635</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 920: 0.26169633865356445</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 921: 0.26137709617614746</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 922: 0.26124075055122375</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 923: 0.26104652881622314</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 924: 0.2608397901058197</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 925: 0.26016730070114136</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 926: 0.25985151529312134</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 927: 0.259538471698761</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 928: 0.2593984603881836</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 929: 0.2592063844203949</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 930: 0.25900888442993164</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 931: 0.2583571672439575</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 932: 0.2580370604991913</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 933: 0.25772541761398315</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 934: 0.25759294629096985</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 935: 0.25739946961402893</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 936: 0.2572041153907776</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 937: 0.2565646469593048</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 938: 0.25624880194664</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 939: 0.25595393776893616</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 940: 0.255830854177475</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 941: 0.25564756989479065</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 942: 0.25545990467071533</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 943: 0.2548166811466217</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 944: 0.2545100748538971</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 945: 0.25420716404914856</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 946: 0.2540774941444397</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 947: 0.2538929283618927</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 948: 0.25371041893959045</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 949: 0.25309208035469055</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 950: 0.25276458263397217</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 951: 0.25246575474739075</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 952: 0.2523382902145386</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 953: 0.25215595960617065</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 954: 0.2519741952419281</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 955: 0.2513558566570282</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 956: 0.2510465383529663</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 957: 0.25074487924575806</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 958: 0.2506274878978729</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 959: 0.25044649839401245</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 960: 0.2502664625644684</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 961: 0.24965031445026398</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 962: 0.249345600605011</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 963: 0.24905556440353394</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 964: 0.2489393651485443</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 965: 0.24875712394714355</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 966: 0.24857734143733978</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 967: 0.24795538187026978</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 968: 0.24765412509441376</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 969: 0.24736660718917847</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 970: 0.24725745618343353</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 971: 0.24706974625587463</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 972: 0.24689200520515442</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 973: 0.24628077447414398</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 974: 0.24597111344337463</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 975: 0.2456812709569931</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 976: 0.24556568264961243</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 977: 0.2453749179840088</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 978: 0.24519789218902588</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 979: 0.24458304047584534</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 980: 0.24428217113018036</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 981: 0.24398890137672424</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 982: 0.24387572705745697</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 983: 0.2436852604150772</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 984: 0.24351027607917786</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 985: 0.24290570616722107</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 986: 0.24260453879833221</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 987: 0.2423216700553894</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 988: 0.24221740663051605</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 989: 0.24203434586524963</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 990: 0.24186863005161285</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 991: 0.24127760529518127</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 992: 0.24156931042671204</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 993: 0.24178262054920197</span>
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(&#34;torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.&#34;)
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 1: 0.20106235444545745</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 2: 0.19391510933637618</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 3: 0.21498359143733978</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 4: 0.20420580804347993</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 5: 0.18980773985385896</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 6: 0.1952504187822342</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 7: 0.19903351962566376</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `fit_mixer` runtime: 12.35 seconds</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Started fitting XGBoost model</span>
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0]     validation_0-mlogloss:0.87488
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-2216:A single GBM iteration takes 0.1 seconds</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Training XGBoost with 131 iterations given 16.473195525407792 seconds constraint</span>
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0]     validation_0-mlogloss:0.87488
[1]     validation_0-mlogloss:0.60290
[2]     validation_0-mlogloss:0.42973
[3]     validation_0-mlogloss:0.31221
[4]     validation_0-mlogloss:0.22983
[5]     validation_0-mlogloss:0.17095
[6]     validation_0-mlogloss:0.12835
[7]     validation_0-mlogloss:0.09724
[8]     validation_0-mlogloss:0.07442
[9]     validation_0-mlogloss:0.05754
[10]    validation_0-mlogloss:0.04502
[11]    validation_0-mlogloss:0.03568
[12]    validation_0-mlogloss:0.02865
[13]    validation_0-mlogloss:0.02334
[14]    validation_0-mlogloss:0.01941
[15]    validation_0-mlogloss:0.01658
[16]    validation_0-mlogloss:0.01459
[17]    validation_0-mlogloss:0.01318
[18]    validation_0-mlogloss:0.01227
[19]    validation_0-mlogloss:0.01171
[20]    validation_0-mlogloss:0.01174
[21]    validation_0-mlogloss:0.01177
[22]    validation_0-mlogloss:0.01182
[23]    validation_0-mlogloss:0.01187
[24]    validation_0-mlogloss:0.01194
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-2216:XGBoost mixer does not have a `partial_fit` implementation</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `fit_mixer` runtime: 0.07 seconds</span>
<span class="ansi-yellow-fg">WARNING:dataprep_ml-2216:Exception: Unspported categorical type for regression when training mixer: &lt;lightwood.mixer.regression.Regression object at 0x7fdbeaba7670&gt;</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Started fitting RandomForest model</span>
<span class="ansi-green-fg">INFO:lightwood-2216:The number of trials (Optuna) is 20.</span>
<span class="ansi-green-fg">INFO:lightwood-2216:init_score: 0.0056968453264924146, optuna_score: 0.013111521012938759</span>
<span class="ansi-green-fg">INFO:lightwood-2216:RandomForest based correlation of (train data): 1.0</span>
<span class="ansi-green-fg">INFO:lightwood-2216:RandomForest based correlation of (dev data): 1.0</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `fit_mixer` runtime: 28.13 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Ensembling the mixer</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Mixer: Neural got accuracy: 0.867</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Mixer: XGBoostMixer got accuracy: 1.0</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Mixer: RandomForest got accuracy: 1.0</span>
<span class="ansi-green-fg">INFO:lightwood-2216:Picked best mixer: RandomForest</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `fit` runtime: 40.77 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:[Learn phase 7/8] - Ensemble analysis</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Analyzing the ensemble of mixers</span>
<span class="ansi-green-fg">INFO:lightwood-2216:The block ICP is now running its analyze() method</span>
<span class="ansi-green-fg">INFO:lightwood-2216:The block ConfStats is now running its analyze() method</span>
<span class="ansi-green-fg">INFO:lightwood-2216:The block AccStats is now running its analyze() method</span>
<span class="ansi-green-fg">INFO:lightwood-2216:The block PermutationFeatureImportance is now running its analyze() method</span>
<span class="ansi-green-fg">INFO:lightwood-2216:[PFI] Using a random sample (1000 rows out of 22).</span>
<span class="ansi-green-fg">INFO:lightwood-2216:[PFI] Set to consider first 10 columns out of 6: [&#39;Population&#39;, &#39;Area (sq. mi.)&#39;, &#39;Pop. Density &#39;, &#39;GDP ($ per capita)&#39;, &#39;Literacy (%)&#39;, &#39;Infant mortality &#39;].</span>
<span class="ansi-green-fg">INFO:lightwood-2216:The block ModelCorrelationHeatmap is now running its analyze() method</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `analyze_ensemble` runtime: 0.39 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:[Learn phase 8/8] - Adjustment on validation requested</span>
<span class="ansi-green-fg">INFO:dataprep_ml-2216:Updating the mixers</span>
/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(&#34;torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.&#34;)
<span class="ansi-green-fg">INFO:lightwood-2216:Loss @ epoch 1: 0.1879083967457215</span>
<span class="ansi-green-fg">INFO:lightwood-2216:XGBoost mixer does not have a `partial_fit` implementation</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `adjust` runtime: 1.78 seconds</span>
<span class="ansi-white-fg">DEBUG:lightwood-2216: `learn` runtime: 43.02 seconds</span>
</pre></div></div>
</div>
<p>Finally, we can visualize the mixer correlation matrix:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">mc</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">runtime_analyzer</span><span class="p">[</span><span class="s1">&#39;mixer_correlation&#39;</span><span class="p">]</span>  <span class="c1"># newly produced insight</span>

<span class="n">mixer_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">predictor</span><span class="o">.</span><span class="n">ensemble</span><span class="o">.</span><span class="n">mixers</span><span class="p">]</span>

<span class="c1"># plotting code</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mc</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;seismic&#39;</span><span class="p">)</span>

<span class="c1"># set ticks</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mc</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mc</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># set tick labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">mixer_names</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">mixer_names</span><span class="p">)</span>

<span class="c1"># show cell values</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mixer_names</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mixer_names</span><span class="p">)):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">3</span><span class="p">),</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_custom_explainer_custom_explainer_20_0.png" src="../../_images/tutorials_custom_explainer_custom_explainer_20_0.png" />
</div>
</div>
<p>Nice! We’ve just added an additional piece of insight regarding the predictor that Lightwood came up with for the task of predicting the Human Development Index of any given country.</p>
<p>What this matrix is telling us is whether predictions of each pair of the mixers stored in the ensemble have a high correlation or not.</p>
<p>This is, of course, a very simple example, but it shows the convenience of such an abstraction within the broader pipeline that Lightwood automates.</p>
<p>For more complex examples, you can check out any of the three core analysis blocks that we use:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">lightwood.analysis.nc.calibrate.ICP</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lightwood.analysis.helpers.acc_stats.AccStats</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lightwood.analysis.helpers.feature_importance.PermutationFeatureImportance</span></code></p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017-2023, MindsDB.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>